{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77722fd",
   "metadata": {},
   "source": [
    "# SARSA Blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a43347be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(seed_list):  2\n",
      "seed_list:  [1, 2]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "\n",
    "seed_list = list(range(1, 11))\n",
    "seed_list = list(range(1, 3))\n",
    "print(\"len(seed_list): \", len(seed_list))\n",
    "print(\"seed_list: \", seed_list)\n",
    "\n",
    "\n",
    "GAMMA = 0.95\n",
    "\n",
    "\n",
    "N_EPISODES = 10_000\n",
    "ALPHA = 0.05\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 5_000 \n",
    "\n",
    "GAMMA = 0.95\n",
    "\n",
    "\n",
    "N_EPISODES = 30_000\n",
    "ALPHA = 0.05\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 5_000 \n",
    "\n",
    "TERMINAL = (\"terminal\", \"terminal\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c4d1012",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Utils\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def epsilon_greedy(Q: Dict[Tuple, float], state, actions, epsilon: float) -> int:\n",
    "    \"\"\"\n",
    "    ε-greedy policy over Q for a single state.\n",
    "    Q is a dict keyed by (state, action).\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(actions)\n",
    "    q_vals = [Q.get((state, a), 0.0) for a in actions]\n",
    "    return int(np.argmax(q_vals))\n",
    "\n",
    "def pad_and_stack(sequences: List[List[float]]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Take a list of 1D sequences of possibly different lengths and\n",
    "    return a 2D array (n_sequences, max_len) padded with NaNs.\n",
    "    \"\"\"\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    arr = np.full((len(sequences), max_len), np.nan, dtype=float)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        arr[i, : len(seq)] = np.array(seq, dtype=float)\n",
    "    return arr\n",
    "\n",
    "def plot_mean_iqr(\n",
    "    values_2d: np.ndarray,\n",
    "    title: str,\n",
    "    ylabel: str,\n",
    "    filename: str,\n",
    "    xlabel: str = \"Episode\",\n",
    "):\n",
    "    \"\"\"\n",
    "    values_2d: shape (n_seeds, n_episodes)\n",
    "    \"\"\"\n",
    "    episodes = np.arange(values_2d.shape[1])\n",
    "    mean = np.nanmean(values_2d, axis=0)\n",
    "    q25, q75 = np.nanpercentile(values_2d, [25, 75], axis=0)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(episodes, mean, label=\"Mean\")\n",
    "    plt.fill_between(episodes, q25, q75, alpha=0.3, label=\"IQR\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Heatmaps (reuse your style)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def plot_value_heatmap(V: Dict, filename: str, title: str):\n",
    "    \"\"\"\n",
    "    V: dict state -> value\n",
    "    Aggregates over usable ace dimension and plots player sum vs dealer card.\n",
    "    \"\"\"\n",
    "    values = np.zeros((22, 11), dtype=float)  # indices: [player, dealer]\n",
    "    counts = np.zeros((22, 11), dtype=float)\n",
    "\n",
    "    for s, v in V.items():\n",
    "        if s == TERMINAL:\n",
    "            continue\n",
    "        player, dealer, ace = s\n",
    "        values[player, dealer] += v\n",
    "        counts[player, dealer] += 1.0\n",
    "\n",
    "    avg_values = np.divide(values, counts, out=np.zeros_like(values), where=counts != 0)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(avg_values[4:22, 1:11], origin=\"lower\", aspect=\"auto\")\n",
    "    plt.colorbar(label=\"Value\")\n",
    "    plt.xlabel(\"Dealer Card (1–10)\")\n",
    "    plt.ylabel(\"Player Sum (4–21)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "def plot_policy_heatmap(pi: Dict, filename: str, title: str):\n",
    "    \"\"\"\n",
    "    pi: dict state -> action (0=stick,1=hit)\n",
    "    Aggregates over usable ace dimension and plots average action.\n",
    "    \"\"\"\n",
    "    policy_map = np.zeros((22, 11), dtype=float)\n",
    "    counts = np.zeros((22, 11), dtype=float)\n",
    "\n",
    "    for s, a in pi.items():\n",
    "        if s == TERMINAL:\n",
    "            continue\n",
    "        player, dealer, ace = s\n",
    "        policy_map[player, dealer] += a\n",
    "        counts[player, dealer] += 1.0\n",
    "\n",
    "    avg_policy = np.divide(policy_map, counts, out=np.zeros_like(policy_map), where=counts != 0)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(avg_policy[4:22, 1:11], origin=\"lower\", aspect=\"auto\", vmin=0, vmax=1)\n",
    "    plt.colorbar(label=\"Action (0=stick, 1=hit, averaged)\")\n",
    "    plt.xlabel(\"Dealer Card (1–10)\")\n",
    "    plt.ylabel(\"Player Sum (4–21)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Q -> policy/value helpers\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def q_to_v_pi_from_dict(\n",
    "    Q: Dict[Tuple, float],\n",
    "    states: List[Tuple],\n",
    "    actions: List[int],\n",
    "):\n",
    "    \"\"\"\n",
    "    From Q(s,a) dict to:\n",
    "      - V(s) = max_a Q(s,a)\n",
    "      - π(s) = argmax_a Q(s,a)\n",
    "    \"\"\"\n",
    "    V = {}\n",
    "    pi = {}\n",
    "    for s in states:\n",
    "        if s == TERMINAL:\n",
    "            continue\n",
    "        q_vals = [Q.get((s, a), 0.0) for a in actions]\n",
    "        best_a = int(np.argmax(q_vals))\n",
    "        V[s] = float(np.max(q_vals))\n",
    "        pi[s] = best_a\n",
    "    V[TERMINAL] = 0.0\n",
    "    # For completeness, we can assign a dummy action to TERMINAL\n",
    "    pi[TERMINAL] = 0\n",
    "    return V, pi\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SARSA\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def run_sarsa_seed(\n",
    "    seed: int,\n",
    "    n_episodes: int,\n",
    "    gamma: float,\n",
    "    alpha: float,\n",
    "    epsilon_start: float,\n",
    "    epsilon_end: float,\n",
    "    epsilon_decay: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Tabular SARSA on Blackjack-v1 (sab=True).\n",
    "\n",
    "    Returns:\n",
    "      - per-episode returns\n",
    "      - per-episode ΔQ (max |ΔQ| in that episode)\n",
    "      - final Q dict\n",
    "      - wall-clock time\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "\n",
    "    actions = [0, 1]  # 0=stick, 1=hit\n",
    "    Q: Dict[Tuple, float] = {}\n",
    "\n",
    "    returns_per_episode: List[float] = []\n",
    "    deltaQ_per_episode: List[float] = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        # Optionally decay epsilon over time\n",
    "        frac = min(1.0, episode / max(1, epsilon_decay))\n",
    "        epsilon = epsilon_start + frac * (epsilon_end - epsilon_start)\n",
    "\n",
    "        state, _ = env.reset(seed=seed + episode)\n",
    "        done = False\n",
    "        a = epsilon_greedy(Q, state, actions, epsilon)\n",
    "\n",
    "        G = 0.0  # return for this episode\n",
    "        max_delta_this_ep = 0.0\n",
    "\n",
    "        while not done:\n",
    "            next_state, r, terminated, truncated, info = env.step(a)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            G += r\n",
    "\n",
    "            # Choose next action using ε-greedy (on-policy)\n",
    "            if not done:\n",
    "                next_a = epsilon_greedy(Q, next_state, actions, epsilon)\n",
    "                target = r + gamma * Q.get((next_state, next_a), 0.0)\n",
    "            else:\n",
    "                next_a = None\n",
    "                target = r\n",
    "\n",
    "            old_q = Q.get((state, a), 0.0)\n",
    "            new_q = old_q + alpha * (target - old_q)\n",
    "            Q[(state, a)] = new_q\n",
    "\n",
    "            delta = abs(new_q - old_q)\n",
    "            if delta > max_delta_this_ep:\n",
    "                max_delta_this_ep = delta\n",
    "\n",
    "            state = next_state\n",
    "            a = next_a if next_a is not None else 0\n",
    "\n",
    "        returns_per_episode.append(G)\n",
    "        deltaQ_per_episode.append(max_delta_this_ep)\n",
    "\n",
    "    wall_clock = time.time() - start_time\n",
    "    env.close()\n",
    "\n",
    "    return {\n",
    "        \"seed\": seed,\n",
    "        \"returns\": returns_per_episode,\n",
    "        \"deltaQs\": deltaQ_per_episode,\n",
    "        \"Q\": Q,\n",
    "        \"wall_clock\": wall_clock,\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c59ee3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states (including terminal): 361\n",
      "Actions: [0, 1]\n",
      "SARSA episodes: 30000\n",
      "SARSA alpha=0.05, gamma=0.95, eps_start=1.0, eps_end=0.05\n",
      "Running SARSA for seed 1...\n",
      "Running SARSA for seed 2...\n",
      "Saved: blackjack_sarsa_learning_curve.pdf\n",
      "Saved: blackjack_sarsa_deltaQ.pdf\n",
      "Saved: blackjack_sarsa_value_heatmap.pdf\n",
      "Saved: blackjack_sarsa_policy_heatmap.pdf\n",
      "Saved: blackjack_sarsa_wallclock_per_seed.pdf\n",
      "\n",
      "=== Per-seed mean return over last 100 episodes ===\n",
      "Seed 1: -0.0300\n",
      "Seed 2: -0.0300\n",
      "\n",
      "=== Wall-clock summary ===\n",
      "Mean wall-clock per seed: 4.4164s ± 0.0608s\n",
      "Total wall-clock over all seeds: 8.8328s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- Enumerate all blackjack states (same as VI/PI code) ----\n",
    "states = []\n",
    "for player in range(4, 22):\n",
    "    for dealer in range(1, 11):\n",
    "        for ace in [False, True]:\n",
    "            states.append((player, dealer, ace))\n",
    "states.append(TERMINAL)\n",
    "\n",
    "actions = [0, 1]  # 0=stick, 1=hit\n",
    "\n",
    "print(f\"Number of states (including terminal): {len(states)}\")\n",
    "print(f\"Actions: {actions}\")\n",
    "print(f\"SARSA episodes: {N_EPISODES}\")\n",
    "print(f\"SARSA alpha={ALPHA}, gamma={GAMMA}, eps_start={EPSILON_START}, eps_end={EPSILON_END}\")\n",
    "\n",
    "# ---- Run SARSA for each seed ----\n",
    "sarsa_results = []\n",
    "for seed in seed_list:\n",
    "    print(f\"Running SARSA for seed {seed}...\")\n",
    "    res = run_sarsa_seed(\n",
    "        seed=seed,\n",
    "        n_episodes=N_EPISODES,\n",
    "        gamma=GAMMA,\n",
    "        alpha=ALPHA,\n",
    "        epsilon_start=EPSILON_START,\n",
    "        epsilon_end=EPSILON_END,\n",
    "        epsilon_decay=EPSILON_DECAY,\n",
    "    )\n",
    "    sarsa_results.append(res)\n",
    "\n",
    "wall_clocks = np.array([r[\"wall_clock\"] for r in sarsa_results])\n",
    "\n",
    "\n",
    "# ---- Aggregate learning curves ----\n",
    "returns_mat = pad_and_stack([r[\"returns\"] for r in sarsa_results])\n",
    "deltaQ_mat = pad_and_stack([r[\"deltaQs\"] for r in sarsa_results])\n",
    "wall_clocks = np.array([r[\"wall_clock\"] for r in sarsa_results])\n",
    "\n",
    "# Learning curve: return vs episodes\n",
    "plot_mean_iqr(\n",
    "    returns_mat,\n",
    "    title=\"Blackjack SARSA: Return vs Episodes\",\n",
    "    ylabel=\"Episode Return\",\n",
    "    filename=\"blackjack_sarsa_learning_curve.pdf\",\n",
    "    xlabel=\"Episode\",\n",
    ")\n",
    "\n",
    "# ΔQ vs episodes\n",
    "plot_mean_iqr(\n",
    "    deltaQ_mat,\n",
    "    title=\"Blackjack SARSA: ΔQ vs Episodes\",\n",
    "    ylabel=\"Max |ΔQ| per Episode\",\n",
    "    filename=\"blackjack_sarsa_deltaQ.pdf\",\n",
    "    xlabel=\"Episode\",\n",
    ")\n",
    "\n",
    "# ---- Final policy & value maps (from Q-table) ----\n",
    "# Use first seed's Q-table (you can change to average over seeds if you want)\n",
    "Q_example = sarsa_results[0][\"Q\"]\n",
    "V_example, pi_example = q_to_v_pi_from_dict(Q_example, states, actions)\n",
    "\n",
    "plot_value_heatmap(\n",
    "    V_example,\n",
    "    filename=\"blackjack_sarsa_value_heatmap.pdf\",\n",
    "    title=\"Blackjack SARSA: Final Value Map (max_a Q)\",\n",
    ")\n",
    "\n",
    "plot_policy_heatmap(\n",
    "    pi_example,\n",
    "    filename=\"blackjack_sarsa_policy_heatmap.pdf\",\n",
    "    title=\"Blackjack SARSA: Final Policy Map (argmax_a Q)\",\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Wall-Clock Time per Seed Plot\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(seed_list, wall_clocks, marker='o')\n",
    "plt.xlabel(\"Seed\")\n",
    "plt.ylabel(\"Wall-Clock Time (s)\")\n",
    "plt.title(\"Blackjack SARSA: Wall-Clock Time per Seed\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"blackjack_sarsa_wallclock_per_seed.pdf\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"Saved: blackjack_sarsa_wallclock_per_seed.pdf\")\n",
    "\n",
    "\n",
    "\n",
    "# ---- Summary ----\n",
    "mean_return_last_100 = np.nanmean(returns_mat[:, -100:], axis=1)\n",
    "print(\"\\n=== Per-seed mean return over last 100 episodes ===\")\n",
    "for seed, mr in zip(seed_list, mean_return_last_100):\n",
    "    print(f\"Seed {seed}: {mr:.4f}\")\n",
    "\n",
    "print(\"\\n=== Wall-clock summary ===\")\n",
    "print(f\"Mean wall-clock per seed: {wall_clocks.mean():.4f}s ± {wall_clocks.std():.4f}s\")\n",
    "print(f\"Total wall-clock over all seeds: {wall_clocks.sum():.4f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl_report_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "959c59e1",
   "metadata": {},
   "source": [
    "# blackjack_vi_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b3b0d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(seed_list):  10\n",
      "seed_list:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Global config\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\n",
    "seed_list = list(range(1,11))\n",
    "print(\"len(seed_list): \", len(seed_list))\n",
    "print(\"seed_list: \", seed_list)\n",
    "\n",
    "GAMMA = 0.98\n",
    "DELTA = 5e-6\n",
    "PI_GAMMA = 0.95\n",
    "\n",
    "TERMINAL = (\"terminal\", \"terminal\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ba7b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Utils\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def estimate_transitions(env, n_samples: int, seed: int = 0):\n",
    "    \"\"\"\n",
    "    Empirical estimation of P(s'|s,a) and R(s,a) using random policy.\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    counts = {}\n",
    "    rewards = {}\n",
    "\n",
    "    actions = [0, 1]\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            a = np.random.choice(actions)\n",
    "            next_state, r, terminated, truncated, info = env.step(a)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            counts.setdefault((state, a, next_state), 0)\n",
    "            counts[(state, a, next_state)] += 1\n",
    "\n",
    "            rewards.setdefault((state, a), [])\n",
    "            rewards[(state, a)].append(r)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    P = {}\n",
    "    R = {}\n",
    "\n",
    "    # Build transition probabilities\n",
    "    for (s, a, s2), c in counts.items():\n",
    "        P.setdefault((s, a), {})\n",
    "        P[(s, a)][s2] = P[(s, a)].get(s2, 0) + c\n",
    "\n",
    "    for key in P:\n",
    "        total = sum(P[key].values())\n",
    "        for s2 in P[key]:\n",
    "            P[key][s2] /= total\n",
    "\n",
    "    # Build mean rewards\n",
    "    for (s, a), rew_list in rewards.items():\n",
    "        R[(s, a)] = float(np.mean(rew_list))\n",
    "\n",
    "    return P, R\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Value Iteration\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def value_iteration(\n",
    "    states,\n",
    "    actions,\n",
    "    P: Dict,\n",
    "    R: Dict,\n",
    "    gamma: float = GAMMA,\n",
    "    delta: float = DELTA,\n",
    "    patience: int = 3,\n",
    "    init_random: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Standard Value Iteration with per-iteration ΔV logging.\n",
    "    If init_random=True, V is randomly initialized (seed-dependent).\n",
    "    \"\"\"\n",
    "    if init_random:\n",
    "        V = {s: float(np.random.randn() * 1e-3) for s in states}\n",
    "    else:\n",
    "        V = {s: 0.0 for s in states}\n",
    "\n",
    "    V[TERMINAL] = 0.0\n",
    "\n",
    "    best_policy = {s: 0 for s in states}\n",
    "    deltas: List[float] = []\n",
    "    no_improve = 0\n",
    "\n",
    "    while True:\n",
    "        max_diff = 0.0\n",
    "        V_new = {}\n",
    "\n",
    "        for s in states:\n",
    "            q_values = []\n",
    "            for a in actions:\n",
    "                q = R.get((s, a), 0.0)\n",
    "                if (s, a) in P:\n",
    "                    for s2, p in P[(s, a)].items():\n",
    "                        if s2 not in V:\n",
    "                            s2 = TERMINAL\n",
    "                        q += gamma * p * V[s2]\n",
    "                q_values.append(q)\n",
    "\n",
    "            best_q = max(q_values)\n",
    "            best_policy[s] = int(np.argmax(q_values))\n",
    "            V_new[s] = best_q\n",
    "\n",
    "            max_diff = max(max_diff, abs(V_new[s] - V[s]))\n",
    "\n",
    "        V = V_new\n",
    "        deltas.append(max_diff)\n",
    "\n",
    "        if max_diff < delta:\n",
    "            no_improve += 1\n",
    "        else:\n",
    "            no_improve = 0\n",
    "\n",
    "        if no_improve >= patience:\n",
    "            break\n",
    "\n",
    "    return V, best_policy, deltas\n",
    "\n",
    "\n",
    "def run_vi_seed(seed, states, actions, P, R, gamma=GAMMA):\n",
    "    set_seed(seed)\n",
    "    start_time = time.time()\n",
    "    V, pi, deltas = value_iteration(states, actions, P, R, gamma, init_random=True)\n",
    "    wall_clock = time.time() - start_time\n",
    "\n",
    "    vals = np.array([v for s, v in V.items() if s != TERMINAL])\n",
    "    avg_return = float(vals.mean())\n",
    "\n",
    "    return {\n",
    "        \"seed\": seed,\n",
    "        \"V\": V,\n",
    "        \"pi\": pi,\n",
    "        \"deltas\": deltas,\n",
    "        \"avg_return\": avg_return,\n",
    "        \"wall_clock\": wall_clock,\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Policy Iteration (with stats)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def policy_evaluation(policy, states, actions, P, R, gamma=PI_GAMMA, delta=DELTA):\n",
    "    V = {s: 0.0 for s in states}\n",
    "    V[TERMINAL] = 0.0\n",
    "\n",
    "    while True:\n",
    "        max_diff = 0.0\n",
    "        V_new = {}\n",
    "\n",
    "        for s in states:\n",
    "            a = policy[s]\n",
    "            q = R.get((s, a), 0.0)\n",
    "            if (s, a) in P:\n",
    "                for s2, p in P[(s, a)].items():\n",
    "                    if s2 not in V:\n",
    "                        s2 = TERMINAL\n",
    "                    q += gamma * p * V[s2]\n",
    "            V_new[s] = q\n",
    "            max_diff = max(max_diff, abs(V_new[s] - V[s]))\n",
    "\n",
    "        V = V_new\n",
    "        if max_diff < delta:\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_iteration_with_stats(\n",
    "    states,\n",
    "    actions,\n",
    "    P,\n",
    "    R,\n",
    "    gamma=PI_GAMMA,\n",
    "):\n",
    "    \"\"\"\n",
    "    Policy Iteration with:\n",
    "      - ΔV_k = max_s |V^{π_k}(s) - V^{π_{k-1}}(s)| per outer iteration\n",
    "      - policy_change_frac_k = fraction of states whose action changes at iteration k\n",
    "    \"\"\"\n",
    "    # random initial policy\n",
    "    policy = {s: np.random.choice(actions) for s in states}\n",
    "\n",
    "    iteration = 0\n",
    "    stable = False\n",
    "    deltaV_list: List[float] = []\n",
    "    policy_change_list: List[float] = []\n",
    "\n",
    "    V_prev = {s: 0.0 for s in states}\n",
    "    V_prev[TERMINAL] = 0.0\n",
    "\n",
    "    non_terminal_states = [s for s in states if s != TERMINAL]\n",
    "    n_non_terminal = len(non_terminal_states)\n",
    "\n",
    "    while not stable:\n",
    "        iteration += 1\n",
    "\n",
    "        # Policy evaluation\n",
    "        V = policy_evaluation(policy, states, actions, P, R, gamma)\n",
    "\n",
    "        # ΔV between this and previous iteration\n",
    "        diffs = [abs(V[s] - V_prev[s]) for s in non_terminal_states]\n",
    "        deltaV = max(diffs) if diffs else 0.0\n",
    "        deltaV_list.append(deltaV)\n",
    "        V_prev = V\n",
    "\n",
    "        # Policy improvement\n",
    "        stable = True\n",
    "        changed_states = 0\n",
    "        for s in non_terminal_states:\n",
    "            old_a = policy[s]\n",
    "            q_vals = []\n",
    "            for a in actions:\n",
    "                q = R.get((s, a), 0.0)\n",
    "                if (s, a) in P:\n",
    "                    for s2, p in P[(s, a)].items():\n",
    "                        if s2 not in V:\n",
    "                            s2 = TERMINAL\n",
    "                        q += gamma * p * V[s2]\n",
    "                q_vals.append(q)\n",
    "\n",
    "            best_a = int(np.argmax(q_vals))\n",
    "            policy[s] = best_a\n",
    "\n",
    "            if best_a != old_a:\n",
    "                changed_states += 1\n",
    "                stable = False\n",
    "\n",
    "        policy_change_frac = changed_states / n_non_terminal if n_non_terminal > 0 else 0.0\n",
    "        policy_change_list.append(policy_change_frac)\n",
    "\n",
    "    return V, policy, deltaV_list, policy_change_list\n",
    "\n",
    "\n",
    "def run_pi_seed(seed, states, actions, P, R, gamma=PI_GAMMA):\n",
    "    set_seed(seed)\n",
    "    start_time = time.time()\n",
    "    V, pi, deltaVs, policy_changes = policy_iteration_with_stats(\n",
    "        states, actions, P, R, gamma\n",
    "    )\n",
    "    wall_clock = time.time() - start_time\n",
    "\n",
    "    vals = np.array([v for s, v in V.items() if s != TERMINAL])\n",
    "    avg_return = float(vals.mean())\n",
    "\n",
    "    return {\n",
    "        \"seed\": seed,\n",
    "        \"V\": V,\n",
    "        \"pi\": pi,\n",
    "        \"deltaVs\": deltaVs,\n",
    "        \"policy_changes\": policy_changes,\n",
    "        \"avg_return\": avg_return,\n",
    "        \"wall_clock\": wall_clock,\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Aggregation helpers\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def pad_and_stack(sequences: List[List[float]]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Take a list of 1D sequences of possibly different lengths and\n",
    "    return a 2D array (n_sequences, max_len) padded with NaNs.\n",
    "    \"\"\"\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    arr = np.full((len(sequences), max_len), np.nan, dtype=float)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        arr[i, : len(seq)] = np.array(seq, dtype=float)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def plot_mean_iqr(\n",
    "    values_2d: np.ndarray,\n",
    "    title: str,\n",
    "    ylabel: str,\n",
    "    filename: str,\n",
    "    xlabel: str = \"Iteration\",\n",
    "):\n",
    "    \"\"\"\n",
    "    values_2d: shape (n_seeds, n_iters)\n",
    "    \"\"\"\n",
    "    iters = np.arange(values_2d.shape[1])\n",
    "    mean = np.nanmean(values_2d, axis=0)\n",
    "    q25, q75 = np.nanpercentile(values_2d, [25, 75], axis=0)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(iters, mean, label=\"Mean\")\n",
    "    plt.fill_between(iters, q25, q75, alpha=0.3, label=\"IQR\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Heatmaps\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def plot_value_heatmap(V: Dict, filename: str, title: str):\n",
    "    \"\"\"\n",
    "    V: dict state -> value\n",
    "    Aggregates over usable ace dimension and plots player sum vs dealer card.\n",
    "    \"\"\"\n",
    "    values = np.zeros((22, 11), dtype=float)  # indices: [player, dealer]\n",
    "    counts = np.zeros((22, 11), dtype=float)\n",
    "\n",
    "    for s, v in V.items():\n",
    "        if s == TERMINAL:\n",
    "            continue\n",
    "        player, dealer, ace = s\n",
    "        values[player, dealer] += v\n",
    "        counts[player, dealer] += 1.0\n",
    "\n",
    "    avg_values = np.divide(values, counts, out=np.zeros_like(values), where=counts != 0)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(avg_values[4:22, 1:11], origin=\"lower\", aspect=\"auto\")\n",
    "    plt.colorbar(label=\"Value\")\n",
    "    plt.xlabel(\"Dealer Card (1–10)\")\n",
    "    plt.ylabel(\"Player Sum (4–21)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "\n",
    "def plot_policy_heatmap(pi: Dict, filename: str, title: str):\n",
    "    \"\"\"\n",
    "    pi: dict state -> action (0=stick,1=hit)\n",
    "    Aggregates over usable ace dimension and plots average action.\n",
    "    \"\"\"\n",
    "    policy_map = np.zeros((22, 11), dtype=float)\n",
    "    counts = np.zeros((22, 11), dtype=float)\n",
    "\n",
    "    for s, a in pi.items():\n",
    "        if s == TERMINAL:\n",
    "            continue\n",
    "        player, dealer, ace = s\n",
    "        policy_map[player, dealer] += a\n",
    "        counts[player, dealer] += 1.0\n",
    "\n",
    "    avg_policy = np.divide(policy_map, counts, out=np.zeros_like(policy_map), where=counts != 0)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(avg_policy[4:22, 1:11], origin=\"lower\", aspect=\"auto\", vmin=0, vmax=1)\n",
    "    plt.colorbar(label=\"Action (0=stick, 1=hit, averaged)\")\n",
    "    plt.xlabel(\"Dealer Card (1–10)\")\n",
    "    plt.ylabel(\"Player Sum (4–21)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902e98ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Main experiment\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    # ---- Build Blackjack MDP model ----\n",
    "    env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "\n",
    "    # Enumerate states\n",
    "    states = []\n",
    "    for player in range(4, 22):\n",
    "        for dealer in range(1, 11):\n",
    "            for ace in [False, True]:\n",
    "                states.append((player, dealer, ace))\n",
    "    states.append(TERMINAL)\n",
    "\n",
    "    actions = [0, 1]  # 0=stick, 1=hit\n",
    "\n",
    "    print(f\"Number of states (including terminal): {len(states)}\")\n",
    "    print(f\"Actions: {actions}\")\n",
    "\n",
    "    # Estimate transitions once (fixed model)\n",
    "    n_samples = 400_000\n",
    "    print(f\"Estimating transitions with {n_samples} samples...\")\n",
    "    P, R = estimate_transitions(env, n_samples=n_samples, seed=0)\n",
    "    print(f\"Estimated |P|={len(P)}, |R|={len(R)}\")\n",
    "\n",
    "    # ---- Run VI over seeds ----\n",
    "    print(\"\\n=== Running Value Iteration over seeds ===\")\n",
    "    vi_results = []\n",
    "    for seed in seed_list:\n",
    "        res = run_vi_seed(seed, states, actions, P, R, gamma=GAMMA)\n",
    "        vi_results.append(res)\n",
    "\n",
    "    vi_returns = np.array([r[\"avg_return\"] for r in vi_results])\n",
    "    vi_wall_clocks = np.array([r[\"wall_clock\"] for r in vi_results])\n",
    "\n",
    "    print(f\"VI: mean return = {vi_returns.mean():.4f} ± {vi_returns.std():.4f}\")\n",
    "    print(\n",
    "        f\"VI: wall-clock mean = {vi_wall_clocks.mean():.4f}s, \"\n",
    "        f\"total = {vi_wall_clocks.sum():.4f}s\"\n",
    "    )\n",
    "\n",
    "    # ΔV vs iterations (VI) with mean + IQR\n",
    "    vi_deltas_mat = pad_and_stack([r[\"deltas\"] for r in vi_results])\n",
    "    plot_mean_iqr(\n",
    "        vi_deltas_mat,\n",
    "        title=\"Blackjack VI: ΔV vs Iterations\",\n",
    "        ylabel=\"Max ΔV\",\n",
    "        filename=\"blackjack_vi_deltaV.pdf\",\n",
    "    )\n",
    "\n",
    "    # ---- Run PI over seeds ----\n",
    "    print(\"\\n=== Running Policy Iteration over seeds ===\")\n",
    "    pi_results = []\n",
    "    for seed in seed_list:\n",
    "        res = run_pi_seed(seed, states, actions, P, R, gamma=PI_GAMMA)\n",
    "        pi_results.append(res)\n",
    "\n",
    "    pi_returns = np.array([r[\"avg_return\"] for r in pi_results])\n",
    "    pi_wall_clocks = np.array([r[\"wall_clock\"] for r in pi_results])\n",
    "\n",
    "    print(f\"PI: mean return = {pi_returns.mean():.4f} ± {pi_returns.std():.4f}\")\n",
    "    print(\n",
    "        f\"PI: wall-clock mean = {pi_wall_clocks.mean():.4f}s, \"\n",
    "        f\"total = {pi_wall_clocks.sum():.4f}s\"\n",
    "    )\n",
    "\n",
    "    # ΔV vs iterations and policy stability (PI)\n",
    "    pi_deltaV_mat = pad_and_stack([r[\"deltaVs\"] for r in pi_results])\n",
    "    pi_polchg_mat = pad_and_stack([r[\"policy_changes\"] for r in pi_results])\n",
    "\n",
    "    # One figure, two subplots (ΔV and policy change fraction)\n",
    "    iters = np.arange(pi_deltaV_mat.shape[1])\n",
    "\n",
    "    delta_mean = np.nanmean(pi_deltaV_mat, axis=0)\n",
    "    delta_q25, delta_q75 = np.nanpercentile(pi_deltaV_mat, [25, 75], axis=0)\n",
    "\n",
    "    pol_mean = np.nanmean(pi_polchg_mat, axis=0)\n",
    "    pol_q25, pol_q75 = np.nanpercentile(pi_polchg_mat, [25, 75], axis=0)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(6, 6))\n",
    "\n",
    "    # ΔV subplot\n",
    "    ax1.plot(iters, delta_mean, label=\"Mean ΔV\")\n",
    "    ax1.fill_between(iters, delta_q25, delta_q75, alpha=0.3, label=\"IQR\")\n",
    "    ax1.set_ylabel(\"Max ΔV\")\n",
    "    ax1.set_title(\"Blackjack PI: ΔV and Policy Stability\")\n",
    "    ax1.grid(True)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Policy stability subplot\n",
    "    ax2.plot(iters, pol_mean, label=\"Mean policy change fraction\")\n",
    "    ax2.fill_between(iters, pol_q25, pol_q75, alpha=0.3, label=\"IQR\")\n",
    "    ax2.set_xlabel(\"Policy Iteration\")\n",
    "    ax2.set_ylabel(\"Fraction of states changed\")\n",
    "    ax2.grid(True)\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"blackjack_pi_deltaV_policy_stability.pdf\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved: blackjack_pi_deltaV_policy_stability.pdf\")\n",
    "\n",
    "    # ---- Final heatmaps (using first seed's solution) ----\n",
    "    vi_V_example = vi_results[0][\"V\"]\n",
    "    vi_pi_example = vi_results[0][\"pi\"]\n",
    "\n",
    "    pi_V_example = pi_results[0][\"V\"]\n",
    "    pi_pi_example = pi_results[0][\"pi\"]\n",
    "\n",
    "    # VI heatmaps\n",
    "    plot_value_heatmap(\n",
    "        vi_V_example,\n",
    "        filename=\"blackjack_vi_value_heatmap.pdf\",\n",
    "        title=\"Blackjack VI: Final Value Function\",\n",
    "    )\n",
    "    plot_policy_heatmap(\n",
    "        vi_pi_example,\n",
    "        filename=\"blackjack_vi_policy_heatmap.pdf\",\n",
    "        title=\"Blackjack VI: Final Greedy Policy\",\n",
    "    )\n",
    "\n",
    "    # PI heatmaps\n",
    "    plot_value_heatmap(\n",
    "        pi_V_example,\n",
    "        filename=\"blackjack_pi_value_heatmap.pdf\",\n",
    "        title=\"Blackjack PI: Final Value Function\",\n",
    "    )\n",
    "    plot_policy_heatmap(\n",
    "        pi_pi_example,\n",
    "        filename=\"blackjack_pi_policy_heatmap.pdf\",\n",
    "        title=\"Blackjack PI: Final Policy\",\n",
    "    )\n",
    "\n",
    "    for res in vi_results:\n",
    "        print(f\"Seed {res['seed']} – VI iterations: {len(res['deltas'])}\")\n",
    "\n",
    "    for res in pi_results:\n",
    "        print(f\"Seed {res['seed']} – PI iterations: {len(res['deltaVs'])}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Wall-Clock Time per Seed Plots (VI and PI)\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    # VI wall-clock vs seed\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(seed_list, vi_wall_clocks, marker='o')\n",
    "    plt.xlabel(\"Seed\")\n",
    "    plt.ylabel(\"Wall-Clock Time (s)\")\n",
    "    plt.title(\"Blackjack VI: Wall-Clock Time per Seed\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"blackjack_vi_wallclock_per_seed.pdf\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved: blackjack_vi_wallclock_per_seed.pdf\")\n",
    "\n",
    "    # PI wall-clock vs seed\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(seed_list, pi_wall_clocks, marker='o')\n",
    "    plt.xlabel(\"Seed\")\n",
    "    plt.ylabel(\"Wall-Clock Time (s)\")\n",
    "    plt.title(\"Blackjack PI: Wall-Clock Time per Seed\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"blackjack_pi_wallclock_per_seed.pdf\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved: blackjack_pi_wallclock_per_seed.pdf\")\n",
    "\n",
    "\n",
    "    # ---- Final report of seeds and wall-clock ----\n",
    "    print(\"\\n=== Summary ===\")\n",
    "    print(f\"Seeds used: {seed_list}\")\n",
    "    print(f\"Total VI wall-clock over all seeds: {vi_wall_clocks.sum():.4f}s\")\n",
    "    print(f\"Total PI wall-clock over all seeds: {pi_wall_clocks.sum():.4f}s\")\n",
    "    print(\n",
    "        f\"Total wall-clock (VI + PI): {(vi_wall_clocks.sum() + pi_wall_clocks.sum()):.4f}s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1e58a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states (including terminal): 361\n",
      "Actions: [0, 1]\n",
      "Estimating transitions with 400000 samples...\n",
      "Estimated |P|=560, |R|=560\n",
      "\n",
      "=== Running Value Iteration over seeds ===\n",
      "VI: mean return = 13.2174 ± 0.0000\n",
      "VI: wall-clock mean = 1.2718s, total = 12.7183s\n",
      "Saved: blackjack_vi_deltaV.pdf\n",
      "\n",
      "=== Running Policy Iteration over seeds ===\n",
      "PI: mean return = 4.9400 ± 0.0000\n",
      "PI: wall-clock mean = 0.3490s, total = 3.4901s\n",
      "Saved: blackjack_pi_deltaV_policy_stability.pdf\n",
      "Saved: blackjack_vi_value_heatmap.pdf\n",
      "Saved: blackjack_vi_policy_heatmap.pdf\n",
      "Saved: blackjack_pi_value_heatmap.pdf\n",
      "Saved: blackjack_pi_policy_heatmap.pdf\n",
      "Seed 1 – VI iterations: 607\n",
      "Seed 2 – VI iterations: 607\n",
      "Seed 3 – VI iterations: 607\n",
      "Seed 4 – VI iterations: 607\n",
      "Seed 5 – VI iterations: 607\n",
      "Seed 6 – VI iterations: 607\n",
      "Seed 7 – VI iterations: 607\n",
      "Seed 8 – VI iterations: 607\n",
      "Seed 9 – VI iterations: 607\n",
      "Seed 10 – VI iterations: 607\n",
      "Seed 1 – PI iterations: 3\n",
      "Seed 2 – PI iterations: 3\n",
      "Seed 3 – PI iterations: 3\n",
      "Seed 4 – PI iterations: 3\n",
      "Seed 5 – PI iterations: 3\n",
      "Seed 6 – PI iterations: 3\n",
      "Seed 7 – PI iterations: 3\n",
      "Seed 8 – PI iterations: 3\n",
      "Seed 9 – PI iterations: 3\n",
      "Seed 10 – PI iterations: 3\n",
      "Saved: blackjack_vi_wallclock_per_seed.pdf\n",
      "Saved: blackjack_pi_wallclock_per_seed.pdf\n",
      "\n",
      "=== Summary ===\n",
      "Seeds used: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Total VI wall-clock over all seeds: 12.7183s\n",
      "Total PI wall-clock over all seeds: 3.4901s\n",
      "Total wall-clock (VI + PI): 16.2084s\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl_report_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad3f759d",
   "metadata": {},
   "source": [
    "# Q-Learning blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb7d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(seed_list):  10\n",
      "seed_list:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "\n",
    "seed_list = list(range(1, 11))   \n",
    "print(\"len(seed_list): \", len(seed_list))\n",
    "print(\"seed_list: \", seed_list)\n",
    "\n",
    "GAMMA = 0.98\n",
    "\n",
    "N_EPISODES = 400_000\n",
    "ALPHA = 0.05\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 5_000\n",
    "\n",
    "TERMINAL = (\"terminal\", \"terminal\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def epsilon_greedy(Q: Dict[Tuple, float], state, actions, epsilon: float) -> int:\n",
    "\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(actions)\n",
    "    q_vals = [Q.get((state, a), 0.0) for a in actions]\n",
    "    return int(np.argmax(q_vals))\n",
    "\n",
    "def pad_and_stack(sequences: List[List[float]]) -> np.ndarray:\n",
    "\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    arr = np.full((len(sequences), max_len), np.nan, dtype=float)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        arr[i, : len(seq)] = np.array(seq, dtype=float)\n",
    "    return arr\n",
    "\n",
    "def plot_mean_iqr(\n",
    "    values_2d: np.ndarray,\n",
    "    title: str,\n",
    "    ylabel: str,\n",
    "    filename: str,\n",
    "    xlabel: str = \"Episode\",\n",
    "):\n",
    "\n",
    "    episodes = np.arange(values_2d.shape[1])\n",
    "    mean = np.nanmean(values_2d, axis=0)\n",
    "    q25, q75 = np.nanpercentile(values_2d, [25, 75], axis=0)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(episodes, mean, label=\"Mean\")\n",
    "    plt.fill_between(episodes, q25, q75, alpha=0.3, label=\"IQR\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "def plot_comparison_mean(\n",
    "    sarsa_mat: np.ndarray,\n",
    "    ql_mat: np.ndarray,\n",
    "    title: str,\n",
    "    ylabel: str,\n",
    "    filename: str,\n",
    "    xlabel: str = \"Episode\",\n",
    "):\n",
    "\n",
    "    n_episodes = min(sarsa_mat.shape[1], ql_mat.shape[1])\n",
    "    episodes = np.arange(n_episodes)\n",
    "\n",
    "    sarsa_mean = np.nanmean(sarsa_mat[:, :n_episodes], axis=0)\n",
    "    ql_mean = np.nanmean(ql_mat[:, :n_episodes], axis=0)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(episodes, sarsa_mean, label=\"SARSA\")\n",
    "    plt.plot(episodes, ql_mean, label=\"Q-Learning\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "\n",
    "def plot_value_heatmap(V: Dict, filename: str, title: str):\n",
    "\n",
    "    values = np.zeros((22, 11), dtype=float)  \n",
    "    counts = np.zeros((22, 11), dtype=float)\n",
    "\n",
    "    for s, v in V.items():\n",
    "        if s == TERMINAL:\n",
    "            continue\n",
    "        player, dealer, ace = s\n",
    "        values[player, dealer] += v\n",
    "        counts[player, dealer] += 1.0\n",
    "\n",
    "    avg_values = np.divide(values, counts, out=np.zeros_like(values), where=counts != 0)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(avg_values[4:22, 1:11], origin=\"lower\", aspect=\"auto\")\n",
    "    plt.colorbar(label=\"Value\")\n",
    "    plt.xlabel(\"Dealer Card (1–10)\")\n",
    "    plt.ylabel(\"Player Sum (4–21)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "def plot_policy_heatmap(pi: Dict, filename: str, title: str):\n",
    "\n",
    "    policy_map = np.zeros((22, 11), dtype=float)\n",
    "    counts = np.zeros((22, 11), dtype=float)\n",
    "\n",
    "    for s, a in pi.items():\n",
    "        if s == TERMINAL:\n",
    "            continue\n",
    "        player, dealer, ace = s\n",
    "        policy_map[player, dealer] += a\n",
    "        counts[player, dealer] += 1.0\n",
    "\n",
    "    avg_policy = np.divide(policy_map, counts, out=np.zeros_like(policy_map), where=counts != 0)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(avg_policy[4:22, 1:11], origin=\"lower\", aspect=\"auto\", vmin=0, vmax=1)\n",
    "    plt.colorbar(label=\"Action (0=stick, 1=hit, averaged)\")\n",
    "    plt.xlabel(\"Dealer Card (1–10)\")\n",
    "    plt.ylabel(\"Player Sum (4–21)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "\n",
    "def q_to_v_pi_from_dict(\n",
    "    Q: Dict[Tuple, float],\n",
    "    states: List[Tuple],\n",
    "    actions: List[int],\n",
    "):\n",
    "    V = {}\n",
    "    pi = {}\n",
    "    for s in states:\n",
    "        if s == TERMINAL:\n",
    "            continue\n",
    "        q_vals = [Q.get((s, a), 0.0) for a in actions]\n",
    "        best_a = int(np.argmax(q_vals))\n",
    "        V[s] = float(np.max(q_vals))\n",
    "        pi[s] = best_a\n",
    "    V[TERMINAL] = 0.0\n",
    "    pi[TERMINAL] = 0\n",
    "    return V, pi\n",
    "\n",
    "\n",
    "\n",
    "def run_sarsa_seed(\n",
    "    seed: int,\n",
    "    n_episodes: int,\n",
    "    gamma: float,\n",
    "    alpha: float,\n",
    "    epsilon_start: float,\n",
    "    epsilon_end: float,\n",
    "    epsilon_decay: int,\n",
    "):\n",
    "\n",
    "    env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "\n",
    "    actions = [0, 1]  # 0=stick, 1=hit\n",
    "    Q: Dict[Tuple, float] = {}\n",
    "\n",
    "    returns_per_episode: List[float] = []\n",
    "    deltaQ_per_episode: List[float] = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        frac = min(1.0, episode / max(1, epsilon_decay))\n",
    "        epsilon = epsilon_start + frac * (epsilon_end - epsilon_start)\n",
    "\n",
    "        state, _ = env.reset(seed=seed + episode)\n",
    "        done = False\n",
    "        a = epsilon_greedy(Q, state, actions, epsilon)\n",
    "\n",
    "        G = 0.0\n",
    "        max_delta_this_ep = 0.0\n",
    "\n",
    "        while not done:\n",
    "            next_state, r, terminated, truncated, info = env.step(a)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            G += r\n",
    "\n",
    "            if not done:\n",
    "                next_a = epsilon_greedy(Q, next_state, actions, epsilon)\n",
    "                target = r + gamma * Q.get((next_state, next_a), 0.0)\n",
    "            else:\n",
    "                next_a = None\n",
    "                target = r\n",
    "\n",
    "            old_q = Q.get((state, a), 0.0)\n",
    "            new_q = old_q + alpha * (target - old_q)\n",
    "            Q[(state, a)] = new_q\n",
    "\n",
    "            delta = abs(new_q - old_q)\n",
    "            if delta > max_delta_this_ep:\n",
    "                max_delta_this_ep = delta\n",
    "\n",
    "            state = next_state\n",
    "            a = next_a if next_a is not None else 0\n",
    "\n",
    "        returns_per_episode.append(G)\n",
    "        deltaQ_per_episode.append(max_delta_this_ep)\n",
    "\n",
    "    wall_clock = time.time() - start_time\n",
    "    env.close()\n",
    "\n",
    "    return {\n",
    "        \"seed\": seed,\n",
    "        \"returns\": returns_per_episode,\n",
    "        \"deltaQs\": deltaQ_per_episode,\n",
    "        \"Q\": Q,\n",
    "        \"wall_clock\": wall_clock,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_q_learning_seed(\n",
    "    seed: int,\n",
    "    n_episodes: int,\n",
    "    gamma: float,\n",
    "    alpha: float,\n",
    "    epsilon_start: float,\n",
    "    epsilon_end: float,\n",
    "    epsilon_decay: int,\n",
    "):\n",
    "\n",
    "    set_seed(seed)\n",
    "    env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "\n",
    "    actions = [0, 1] \n",
    "    Q: Dict[Tuple, float] = {}\n",
    "\n",
    "    returns_per_episode: List[float] = []\n",
    "    deltaQ_per_episode: List[float] = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        frac = min(1.0, episode / max(1, epsilon_decay))\n",
    "        epsilon = epsilon_start + frac * (epsilon_end - epsilon_start)\n",
    "\n",
    "        state, _ = env.reset(seed=seed + 10_000 + episode)  \n",
    "        done = False\n",
    "        G = 0.0\n",
    "        max_delta_this_ep = 0.0\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            a = epsilon_greedy(Q, state, actions, epsilon)\n",
    "            next_state, r, terminated, truncated, info = env.step(a)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            G += r\n",
    "\n",
    "\n",
    "            if not done:\n",
    "                q_next_vals = [Q.get((next_state, a2), 0.0) for a2 in actions]\n",
    "                best_next = np.max(q_next_vals)\n",
    "                target = r + gamma * best_next\n",
    "            else:\n",
    "                target = r\n",
    "\n",
    "            old_q = Q.get((state, a), 0.0)\n",
    "            new_q = old_q + alpha * (target - old_q)\n",
    "            Q[(state, a)] = new_q\n",
    "\n",
    "            delta = abs(new_q - old_q)\n",
    "            if delta > max_delta_this_ep:\n",
    "                max_delta_this_ep = delta\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        returns_per_episode.append(G)\n",
    "        deltaQ_per_episode.append(max_delta_this_ep)\n",
    "\n",
    "    wall_clock = time.time() - start_time\n",
    "    env.close()\n",
    "\n",
    "    return {\n",
    "        \"seed\": seed,\n",
    "        \"returns\": returns_per_episode,\n",
    "        \"deltaQs\": deltaQ_per_episode,\n",
    "        \"Q\": Q,\n",
    "        \"wall_clock\": wall_clock,\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1bd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states (including terminal): 361\n",
      "Actions: [0, 1]\n",
      "Episodes: 400000\n",
      "alpha=0.05, gamma=0.98, eps_start=1.0, eps_end=0.05\n",
      "Running SARSA for seed 1...\n",
      "Running SARSA for seed 2...\n",
      "Running SARSA for seed 3...\n",
      "Running SARSA for seed 4...\n",
      "Running SARSA for seed 5...\n",
      "Running SARSA for seed 6...\n",
      "Running SARSA for seed 7...\n",
      "Running SARSA for seed 8...\n",
      "Running SARSA for seed 9...\n",
      "Running SARSA for seed 10...\n",
      "Running Q-Learning for seed 1...\n",
      "Running Q-Learning for seed 2...\n",
      "Running Q-Learning for seed 3...\n",
      "Running Q-Learning for seed 4...\n",
      "Running Q-Learning for seed 5...\n",
      "Running Q-Learning for seed 6...\n",
      "Running Q-Learning for seed 7...\n",
      "Running Q-Learning for seed 8...\n",
      "Running Q-Learning for seed 9...\n",
      "Running Q-Learning for seed 10...\n",
      "Saved: blackjack_sarsa_learning_curve.pdf\n",
      "Saved: blackjack_qlearning_learning_curve.pdf\n",
      "Saved: blackjack_sarsa_deltaQ.pdf\n",
      "Saved: blackjack_qlearning_deltaQ.pdf\n",
      "Saved: blackjack_sarsa_vs_qlearning_learning_curve.pdf\n",
      "Saved: blackjack_qlearning_value_heatmap.pdf\n",
      "Saved: blackjack_qlearning_policy_heatmap.pdf\n",
      "Saved: blackjack_sarsa_value_heatmap.pdf\n",
      "Saved: blackjack_sarsa_policy_heatmap.pdf\n",
      "Saved: blackjack_sarsa_vs_qlearning_wallclock_per_seed.pdf\n",
      "\n",
      "=== Per-seed mean return over last 100 episodes (SARSA) ===\n",
      "SARSA Seed 1: -0.0400\n",
      "SARSA Seed 2: -0.0500\n",
      "SARSA Seed 3: -0.0700\n",
      "SARSA Seed 4: -0.0400\n",
      "SARSA Seed 5: -0.0300\n",
      "SARSA Seed 6: 0.0200\n",
      "SARSA Seed 7: -0.0700\n",
      "SARSA Seed 8: -0.0500\n",
      "SARSA Seed 9: -0.0600\n",
      "SARSA Seed 10: 0.0100\n",
      "\n",
      "=== Per-seed mean return over last 100 episodes (Q-Learning) ===\n",
      "Q-Learning Seed 1: -0.2700\n",
      "Q-Learning Seed 2: -0.2800\n",
      "Q-Learning Seed 3: -0.2200\n",
      "Q-Learning Seed 4: -0.2500\n",
      "Q-Learning Seed 5: -0.2100\n",
      "Q-Learning Seed 6: -0.2500\n",
      "Q-Learning Seed 7: -0.2400\n",
      "Q-Learning Seed 8: -0.2800\n",
      "Q-Learning Seed 9: -0.2800\n",
      "Q-Learning Seed 10: -0.2500\n",
      "\n",
      "=== Wall-clock summary (SARSA) ===\n",
      "Mean wall-clock per seed: 56.3074s ± 0.5039s\n",
      "Total wall-clock over all seeds: 563.0744s\n",
      "\n",
      "=== Wall-clock summary (Q-Learning) ===\n",
      "Mean wall-clock per seed: 64.0315s ± 2.5561s\n",
      "Total wall-clock over all seeds: 640.3146s\n"
     ]
    }
   ],
   "source": [
    "states = []\n",
    "for player in range(4, 22):\n",
    "    for dealer in range(1, 11):\n",
    "        for ace in [False, True]:\n",
    "            states.append((player, dealer, ace))\n",
    "states.append(TERMINAL)\n",
    "\n",
    "actions = [0, 1] \n",
    "\n",
    "print(f\"Number of states (including terminal): {len(states)}\")\n",
    "print(f\"Actions: {actions}\")\n",
    "print(f\"Episodes: {N_EPISODES}\")\n",
    "print(f\"alpha={ALPHA}, gamma={GAMMA}, eps_start={EPSILON_START}, eps_end={EPSILON_END}\")\n",
    "\n",
    "\n",
    "sarsa_results = []\n",
    "for seed in seed_list:\n",
    "    print(f\"Running SARSA for seed {seed}...\")\n",
    "    res = run_sarsa_seed(\n",
    "        seed=seed,\n",
    "        n_episodes=N_EPISODES,\n",
    "        gamma=GAMMA,\n",
    "        alpha=ALPHA,\n",
    "        epsilon_start=EPSILON_START,\n",
    "        epsilon_end=EPSILON_END,\n",
    "        epsilon_decay=EPSILON_DECAY,\n",
    "    )\n",
    "    sarsa_results.append(res)\n",
    "\n",
    "\n",
    "qlearning_results = []\n",
    "for seed in seed_list:\n",
    "    print(f\"Running Q-Learning for seed {seed}...\")\n",
    "    res = run_q_learning_seed(\n",
    "        seed=seed,\n",
    "        n_episodes=N_EPISODES,\n",
    "        gamma=GAMMA,\n",
    "        alpha=ALPHA,\n",
    "        epsilon_start=EPSILON_START,\n",
    "        epsilon_end=EPSILON_END,\n",
    "        epsilon_decay=EPSILON_DECAY,\n",
    "    )\n",
    "    qlearning_results.append(res)\n",
    "\n",
    "\n",
    "\n",
    "returns_mat_sarsa = pad_and_stack([r[\"returns\"] for r in sarsa_results])\n",
    "deltaQ_mat_sarsa = pad_and_stack([r[\"deltaQs\"] for r in sarsa_results])\n",
    "wall_clocks_sarsa = np.array([r[\"wall_clock\"] for r in sarsa_results])\n",
    "\n",
    "\n",
    "returns_mat_ql = pad_and_stack([r[\"returns\"] for r in qlearning_results])\n",
    "deltaQ_mat_ql = pad_and_stack([r[\"deltaQs\"] for r in qlearning_results])\n",
    "wall_clocks_ql = np.array([r[\"wall_clock\"] for r in qlearning_results])\n",
    "\n",
    "\n",
    "plot_mean_iqr(\n",
    "    returns_mat_sarsa,\n",
    "    title=\"Blackjack SARSA: Return vs Episodes\",\n",
    "    ylabel=\"Episode Return\",\n",
    "    filename=\"blackjack_sarsa_learning_curve.pdf\",\n",
    "    xlabel=\"Episode\",\n",
    ")\n",
    "\n",
    "plot_mean_iqr(\n",
    "    returns_mat_ql,\n",
    "    title=\"Blackjack Q-Learning: Return vs Episodes\",\n",
    "    ylabel=\"Episode Return\",\n",
    "    filename=\"blackjack_qlearning_learning_curve.pdf\",\n",
    "    xlabel=\"Episode\",\n",
    ")\n",
    "\n",
    "plot_mean_iqr(\n",
    "    deltaQ_mat_sarsa,\n",
    "    title=\"Blackjack SARSA: ΔQ vs Episodes\",\n",
    "    ylabel=\"Max |ΔQ| per Episode\",\n",
    "    filename=\"blackjack_sarsa_deltaQ.pdf\",\n",
    "    xlabel=\"Episode\",\n",
    ")\n",
    "\n",
    "plot_mean_iqr(\n",
    "    deltaQ_mat_ql,\n",
    "    title=\"Blackjack Q-Learning: ΔQ vs Episodes\",\n",
    "    ylabel=\"Max |ΔQ| per Episode\",\n",
    "    filename=\"blackjack_qlearning_deltaQ.pdf\",\n",
    "    xlabel=\"Episode\",\n",
    ")\n",
    "\n",
    "plot_comparison_mean(\n",
    "    returns_mat_sarsa,\n",
    "    returns_mat_ql,\n",
    "    title=\"Blackjack: SARSA vs Q-Learning (Return vs Episodes)\",\n",
    "    ylabel=\"Episode Return\",\n",
    "    filename=\"blackjack_sarsa_vs_qlearning_learning_curve.pdf\",\n",
    "    xlabel=\"Episode\",\n",
    ")\n",
    "\n",
    "\n",
    "Q_ql_example = qlearning_results[0][\"Q\"]\n",
    "V_ql_example, pi_ql_example = q_to_v_pi_from_dict(Q_ql_example, states, actions)\n",
    "\n",
    "plot_value_heatmap(\n",
    "    V_ql_example,\n",
    "    filename=\"blackjack_qlearning_value_heatmap.pdf\",\n",
    "    title=\"Blackjack Q-Learning: Final Value Map (max_a Q)\",\n",
    ")\n",
    "\n",
    "plot_policy_heatmap(\n",
    "    pi_ql_example,\n",
    "    filename=\"blackjack_qlearning_policy_heatmap.pdf\",\n",
    "    title=\"Blackjack Q-Learning: Final Policy Map (argmax_a Q)\",\n",
    ")\n",
    "\n",
    "Q_sarsa_example = sarsa_results[0][\"Q\"]\n",
    "V_sarsa_example, pi_sarsa_example = q_to_v_pi_from_dict(Q_sarsa_example, states, actions)\n",
    "\n",
    "plot_value_heatmap(\n",
    "    V_sarsa_example,\n",
    "    filename=\"blackjack_sarsa_value_heatmap.pdf\",\n",
    "    title=\"Blackjack SARSA: Final Value Map (max_a Q)\",\n",
    ")\n",
    "\n",
    "plot_policy_heatmap(\n",
    "    pi_sarsa_example,\n",
    "    filename=\"blackjack_sarsa_policy_heatmap.pdf\",\n",
    "    title=\"Blackjack SARSA: Final Policy Map (argmax_a Q)\",\n",
    ")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(seed_list, wall_clocks_sarsa, marker='o', label=\"SARSA\")\n",
    "plt.plot(seed_list, wall_clocks_ql, marker='s', label=\"Q-Learning\")\n",
    "plt.xlabel(\"Seed\")\n",
    "plt.ylabel(\"Wall-Clock Time (s)\")\n",
    "plt.title(\"Blackjack: Wall-Clock Time per Seed (SARSA vs Q-Learning)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"blackjack_sarsa_vs_qlearning_wallclock_per_seed.pdf\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"Saved: blackjack_sarsa_vs_qlearning_wallclock_per_seed.pdf\")\n",
    "\n",
    "mean_return_last_100_sarsa = np.nanmean(returns_mat_sarsa[:, -100:], axis=1)\n",
    "mean_return_last_100_ql = np.nanmean(returns_mat_ql[:, -100:], axis=1)\n",
    "\n",
    "print(\"\\n=== Per-seed mean return over last 100 episodes (SARSA) ===\")\n",
    "for seed, mr in zip(seed_list, mean_return_last_100_sarsa):\n",
    "    print(f\"SARSA Seed {seed}: {mr:.4f}\")\n",
    "\n",
    "print(\"\\n=== Per-seed mean return over last 100 episodes (Q-Learning) ===\")\n",
    "for seed, mr in zip(seed_list, mean_return_last_100_ql):\n",
    "    print(f\"Q-Learning Seed {seed}: {mr:.4f}\")\n",
    "\n",
    "print(\"\\n=== Wall-clock summary (SARSA) ===\")\n",
    "print(f\"Mean wall-clock per seed: {wall_clocks_sarsa.mean():.4f}s ± {wall_clocks_sarsa.std():.4f}s\")\n",
    "print(f\"Total wall-clock over all seeds: {wall_clocks_sarsa.sum():.4f}s\")\n",
    "\n",
    "print(\"\\n=== Wall-clock summary (Q-Learning) ===\")\n",
    "print(f\"Mean wall-clock per seed: {wall_clocks_ql.mean():.4f}s ± {wall_clocks_ql.std():.4f}s\")\n",
    "print(f\"Total wall-clock over all seeds: {wall_clocks_ql.sum():.4f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl_report_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

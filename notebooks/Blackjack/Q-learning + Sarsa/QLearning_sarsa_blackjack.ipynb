{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad3f759d",
   "metadata": {},
   "source": [
    "# Q-Learning blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09fb7d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(seed_list):  2\n",
      "seed_list:  [1, 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Global config\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "seed_list = list(range(1, 3))   # use 1..10 for final experiments if you want\n",
    "print(\"len(seed_list): \", len(seed_list))\n",
    "print(\"seed_list: \", seed_list)\n",
    "\n",
    "GAMMA = 0.98\n",
    "\n",
    "N_EPISODES = 30_000\n",
    "ALPHA = 0.05\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 5_000\n",
    "\n",
    "TERMINAL = (\"terminal\", \"terminal\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1937d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def epsilon_greedy(Q: Dict[Tuple, float], state, actions, epsilon: float) -> int:\n",
    "    \"\"\"\n",
    "    ε-greedy policy over Q for a single state.\n",
    "    Q is a dict keyed by (state, action).\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(actions)\n",
    "    q_vals = [Q.get((state, a), 0.0) for a in actions]\n",
    "    return int(np.argmax(q_vals))\n",
    "\n",
    "def pad_and_stack(sequences: List[List[float]]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Take a list of 1D sequences of possibly different lengths and\n",
    "    return a 2D array (n_sequences, max_len) padded with NaNs.\n",
    "    \"\"\"\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    arr = np.full((len(sequences), max_len), np.nan, dtype=float)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        arr[i, : len(seq)] = np.array(seq, dtype=float)\n",
    "    return arr\n",
    "\n",
    "def plot_mean_iqr(\n",
    "    values_2d: np.ndarray,\n",
    "    title: str,\n",
    "    ylabel: str,\n",
    "    filename: str,\n",
    "    xlabel: str = \"Episode\",\n",
    "):\n",
    "    \"\"\"\n",
    "    values_2d: shape (n_seeds, n_episodes)\n",
    "    \"\"\"\n",
    "    episodes = np.arange(values_2d.shape[1])\n",
    "    mean = np.nanmean(values_2d, axis=0)\n",
    "    q25, q75 = np.nanpercentile(values_2d, [25, 75], axis=0)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(episodes, mean, label=\"Mean\")\n",
    "    plt.fill_between(episodes, q25, q75, alpha=0.3, label=\"IQR\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "def plot_comparison_mean(\n",
    "    sarsa_mat: np.ndarray,\n",
    "    ql_mat: np.ndarray,\n",
    "    title: str,\n",
    "    ylabel: str,\n",
    "    filename: str,\n",
    "    xlabel: str = \"Episode\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot mean return vs episodes for SARSA and Q-Learning on one figure.\n",
    "    \"\"\"\n",
    "    n_episodes = min(sarsa_mat.shape[1], ql_mat.shape[1])\n",
    "    episodes = np.arange(n_episodes)\n",
    "\n",
    "    sarsa_mean = np.nanmean(sarsa_mat[:, :n_episodes], axis=0)\n",
    "    ql_mean = np.nanmean(ql_mat[:, :n_episodes], axis=0)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(episodes, sarsa_mean, label=\"SARSA\")\n",
    "    plt.plot(episodes, ql_mean, label=\"Q-Learning\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Heatmaps\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def plot_value_heatmap(V: Dict, filename: str, title: str):\n",
    "    \"\"\"\n",
    "    V: dict state -> value\n",
    "    Aggregates over usable ace dimension and plots player sum vs dealer card.\n",
    "    \"\"\"\n",
    "    values = np.zeros((22, 11), dtype=float)  # indices: [player, dealer]\n",
    "    counts = np.zeros((22, 11), dtype=float)\n",
    "\n",
    "    for s, v in V.items():\n",
    "        if s == TERMINAL:\n",
    "            continue\n",
    "        player, dealer, ace = s\n",
    "        values[player, dealer] += v\n",
    "        counts[player, dealer] += 1.0\n",
    "\n",
    "    avg_values = np.divide(values, counts, out=np.zeros_like(values), where=counts != 0)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(avg_values[4:22, 1:11], origin=\"lower\", aspect=\"auto\")\n",
    "    plt.colorbar(label=\"Value\")\n",
    "    plt.xlabel(\"Dealer Card (1–10)\")\n",
    "    plt.ylabel(\"Player Sum (4–21)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "def plot_policy_heatmap(pi: Dict, filename: str, title: str):\n",
    "    \"\"\"\n",
    "    pi: dict state -> action (0=stick,1=hit)\n",
    "    Aggregates over usable ace dimension and plots average action.\n",
    "    \"\"\"\n",
    "    policy_map = np.zeros((22, 11), dtype=float)\n",
    "    counts = np.zeros((22, 11), dtype=float)\n",
    "\n",
    "    for s, a in pi.items():\n",
    "        if s == TERMINAL:\n",
    "            continue\n",
    "        player, dealer, ace = s\n",
    "        policy_map[player, dealer] += a\n",
    "        counts[player, dealer] += 1.0\n",
    "\n",
    "    avg_policy = np.divide(policy_map, counts, out=np.zeros_like(policy_map), where=counts != 0)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(avg_policy[4:22, 1:11], origin=\"lower\", aspect=\"auto\", vmin=0, vmax=1)\n",
    "    plt.colorbar(label=\"Action (0=stick, 1=hit, averaged)\")\n",
    "    plt.xlabel(\"Dealer Card (1–10)\")\n",
    "    plt.ylabel(\"Player Sum (4–21)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Q -> policy/value helpers\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def q_to_v_pi_from_dict(\n",
    "    Q: Dict[Tuple, float],\n",
    "    states: List[Tuple],\n",
    "    actions: List[int],\n",
    "):\n",
    "    \"\"\"\n",
    "    From Q(s,a) dict to:\n",
    "      - V(s) = max_a Q(s,a)\n",
    "      - π(s) = argmax_a Q(s,a)\n",
    "    \"\"\"\n",
    "    V = {}\n",
    "    pi = {}\n",
    "    for s in states:\n",
    "        if s == TERMINAL:\n",
    "            continue\n",
    "        q_vals = [Q.get((s, a), 0.0) for a in actions]\n",
    "        best_a = int(np.argmax(q_vals))\n",
    "        V[s] = float(np.max(q_vals))\n",
    "        pi[s] = best_a\n",
    "    V[TERMINAL] = 0.0\n",
    "    pi[TERMINAL] = 0\n",
    "    return V, pi\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SARSA\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def run_sarsa_seed(\n",
    "    seed: int,\n",
    "    n_episodes: int,\n",
    "    gamma: float,\n",
    "    alpha: float,\n",
    "    epsilon_start: float,\n",
    "    epsilon_end: float,\n",
    "    epsilon_decay: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Tabular SARSA on Blackjack-v1 (sab=True).\n",
    "\n",
    "    Returns:\n",
    "      - per-episode returns\n",
    "      - per-episode ΔQ (max |ΔQ| in that episode)\n",
    "      - final Q dict\n",
    "      - wall-clock time\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "\n",
    "    actions = [0, 1]  # 0=stick, 1=hit\n",
    "    Q: Dict[Tuple, float] = {}\n",
    "\n",
    "    returns_per_episode: List[float] = []\n",
    "    deltaQ_per_episode: List[float] = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        # Decay epsilon over time\n",
    "        frac = min(1.0, episode / max(1, epsilon_decay))\n",
    "        epsilon = epsilon_start + frac * (epsilon_end - epsilon_start)\n",
    "\n",
    "        state, _ = env.reset(seed=seed + episode)\n",
    "        done = False\n",
    "        a = epsilon_greedy(Q, state, actions, epsilon)\n",
    "\n",
    "        G = 0.0\n",
    "        max_delta_this_ep = 0.0\n",
    "\n",
    "        while not done:\n",
    "            next_state, r, terminated, truncated, info = env.step(a)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            G += r\n",
    "\n",
    "            # On-policy target (SARSA)\n",
    "            if not done:\n",
    "                next_a = epsilon_greedy(Q, next_state, actions, epsilon)\n",
    "                target = r + gamma * Q.get((next_state, next_a), 0.0)\n",
    "            else:\n",
    "                next_a = None\n",
    "                target = r\n",
    "\n",
    "            old_q = Q.get((state, a), 0.0)\n",
    "            new_q = old_q + alpha * (target - old_q)\n",
    "            Q[(state, a)] = new_q\n",
    "\n",
    "            delta = abs(new_q - old_q)\n",
    "            if delta > max_delta_this_ep:\n",
    "                max_delta_this_ep = delta\n",
    "\n",
    "            state = next_state\n",
    "            a = next_a if next_a is not None else 0\n",
    "\n",
    "        returns_per_episode.append(G)\n",
    "        deltaQ_per_episode.append(max_delta_this_ep)\n",
    "\n",
    "    wall_clock = time.time() - start_time\n",
    "    env.close()\n",
    "\n",
    "    return {\n",
    "        \"seed\": seed,\n",
    "        \"returns\": returns_per_episode,\n",
    "        \"deltaQs\": deltaQ_per_episode,\n",
    "        \"Q\": Q,\n",
    "        \"wall_clock\": wall_clock,\n",
    "    }\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Q-Learning (off-policy, no Double Q)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def run_q_learning_seed(\n",
    "    seed: int,\n",
    "    n_episodes: int,\n",
    "    gamma: float,\n",
    "    alpha: float,\n",
    "    epsilon_start: float,\n",
    "    epsilon_end: float,\n",
    "    epsilon_decay: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Tabular Q-Learning on Blackjack-v1 (sab=True).\n",
    "\n",
    "    Returns:\n",
    "      - per-episode returns\n",
    "      - per-episode ΔQ (max |ΔQ| in that episode)\n",
    "      - final Q dict\n",
    "      - wall-clock time\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "\n",
    "    actions = [0, 1]  # 0=stick, 1=hit\n",
    "    Q: Dict[Tuple, float] = {}\n",
    "\n",
    "    returns_per_episode: List[float] = []\n",
    "    deltaQ_per_episode: List[float] = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        # Decay epsilon over time\n",
    "        frac = min(1.0, episode / max(1, epsilon_decay))\n",
    "        epsilon = epsilon_start + frac * (epsilon_end - epsilon_start)\n",
    "\n",
    "        state, _ = env.reset(seed=seed + 10_000 + episode)  # shift seeds to differ from SARSA\n",
    "        done = False\n",
    "        G = 0.0\n",
    "        max_delta_this_ep = 0.0\n",
    "\n",
    "        while not done:\n",
    "            # Behaviour: ε-greedy on current Q\n",
    "            a = epsilon_greedy(Q, state, actions, epsilon)\n",
    "            next_state, r, terminated, truncated, info = env.step(a)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            G += r\n",
    "\n",
    "            # Off-policy target: max_a' Q(next_state, a')\n",
    "            if not done:\n",
    "                q_next_vals = [Q.get((next_state, a2), 0.0) for a2 in actions]\n",
    "                best_next = np.max(q_next_vals)\n",
    "                target = r + gamma * best_next\n",
    "            else:\n",
    "                target = r\n",
    "\n",
    "            old_q = Q.get((state, a), 0.0)\n",
    "            new_q = old_q + alpha * (target - old_q)\n",
    "            Q[(state, a)] = new_q\n",
    "\n",
    "            delta = abs(new_q - old_q)\n",
    "            if delta > max_delta_this_ep:\n",
    "                max_delta_this_ep = delta\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        returns_per_episode.append(G)\n",
    "        deltaQ_per_episode.append(max_delta_this_ep)\n",
    "\n",
    "    wall_clock = time.time() - start_time\n",
    "    env.close()\n",
    "\n",
    "    return {\n",
    "        \"seed\": seed,\n",
    "        \"returns\": returns_per_episode,\n",
    "        \"deltaQs\": deltaQ_per_episode,\n",
    "        \"Q\": Q,\n",
    "        \"wall_clock\": wall_clock,\n",
    "    }\n",
    "\n",
    "# %%\n",
    "# ---- Enumerate all blackjack states (same as VI/PI code) ----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe1bd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states (including terminal): 361\n",
      "Actions: [0, 1]\n",
      "Episodes: 30000\n",
      "alpha=0.05, gamma=0.98, eps_start=1.0, eps_end=0.05\n",
      "Running SARSA for seed 1...\n",
      "Running SARSA for seed 2...\n",
      "Running Q-Learning for seed 1...\n",
      "Running Q-Learning for seed 2...\n",
      "Saved: blackjack_sarsa_learning_curve.pdf\n",
      "Saved: blackjack_qlearning_learning_curve.pdf\n",
      "Saved: blackjack_sarsa_deltaQ.pdf\n",
      "Saved: blackjack_qlearning_deltaQ.pdf\n",
      "Saved: blackjack_sarsa_vs_qlearning_learning_curve.pdf\n",
      "Saved: blackjack_qlearning_value_heatmap.pdf\n",
      "Saved: blackjack_qlearning_policy_heatmap.pdf\n",
      "Saved: blackjack_sarsa_value_heatmap.pdf\n",
      "Saved: blackjack_sarsa_policy_heatmap.pdf\n",
      "Saved: blackjack_sarsa_vs_qlearning_wallclock_per_seed.pdf\n",
      "\n",
      "=== Per-seed mean return over last 100 episodes (SARSA) ===\n",
      "SARSA Seed 1: 0.0000\n",
      "SARSA Seed 2: -0.0700\n",
      "\n",
      "=== Per-seed mean return over last 100 episodes (Q-Learning) ===\n",
      "Q-Learning Seed 1: -0.0500\n",
      "Q-Learning Seed 2: -0.0600\n",
      "\n",
      "=== Wall-clock summary (SARSA) ===\n",
      "Mean wall-clock per seed: 5.0005s ± 0.0338s\n",
      "Total wall-clock over all seeds: 10.0011s\n",
      "\n",
      "=== Wall-clock summary (Q-Learning) ===\n",
      "Mean wall-clock per seed: 5.7881s ± 0.6166s\n",
      "Total wall-clock over all seeds: 11.5762s\n"
     ]
    }
   ],
   "source": [
    "states = []\n",
    "for player in range(4, 22):\n",
    "    for dealer in range(1, 11):\n",
    "        for ace in [False, True]:\n",
    "            states.append((player, dealer, ace))\n",
    "states.append(TERMINAL)\n",
    "\n",
    "actions = [0, 1]  # 0=stick, 1=hit\n",
    "\n",
    "print(f\"Number of states (including terminal): {len(states)}\")\n",
    "print(f\"Actions: {actions}\")\n",
    "print(f\"Episodes: {N_EPISODES}\")\n",
    "print(f\"alpha={ALPHA}, gamma={GAMMA}, eps_start={EPSILON_START}, eps_end={EPSILON_END}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Run SARSA for each seed\n",
    "# ---------------------------------------------------------\n",
    "sarsa_results = []\n",
    "for seed in seed_list:\n",
    "    print(f\"Running SARSA for seed {seed}...\")\n",
    "    res = run_sarsa_seed(\n",
    "        seed=seed,\n",
    "        n_episodes=N_EPISODES,\n",
    "        gamma=GAMMA,\n",
    "        alpha=ALPHA,\n",
    "        epsilon_start=EPSILON_START,\n",
    "        epsilon_end=EPSILON_END,\n",
    "        epsilon_decay=EPSILON_DECAY,\n",
    "    )\n",
    "    sarsa_results.append(res)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Run Q-Learning for each seed\n",
    "# ---------------------------------------------------------\n",
    "qlearning_results = []\n",
    "for seed in seed_list:\n",
    "    print(f\"Running Q-Learning for seed {seed}...\")\n",
    "    res = run_q_learning_seed(\n",
    "        seed=seed,\n",
    "        n_episodes=N_EPISODES,\n",
    "        gamma=GAMMA,\n",
    "        alpha=ALPHA,\n",
    "        epsilon_start=EPSILON_START,\n",
    "        epsilon_end=EPSILON_END,\n",
    "        epsilon_decay=EPSILON_DECAY,\n",
    "    )\n",
    "    qlearning_results.append(res)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Aggregate learning curves and ΔQ\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# SARSA\n",
    "returns_mat_sarsa = pad_and_stack([r[\"returns\"] for r in sarsa_results])\n",
    "deltaQ_mat_sarsa = pad_and_stack([r[\"deltaQs\"] for r in sarsa_results])\n",
    "wall_clocks_sarsa = np.array([r[\"wall_clock\"] for r in sarsa_results])\n",
    "\n",
    "# Q-Learning\n",
    "returns_mat_ql = pad_and_stack([r[\"returns\"] for r in qlearning_results])\n",
    "deltaQ_mat_ql = pad_and_stack([r[\"deltaQs\"] for r in qlearning_results])\n",
    "wall_clocks_ql = np.array([r[\"wall_clock\"] for r in qlearning_results])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Learning curves: return vs episodes\n",
    "# ---------------------------------------------------------\n",
    "plot_mean_iqr(\n",
    "    returns_mat_sarsa,\n",
    "    title=\"Blackjack SARSA: Return vs Episodes\",\n",
    "    ylabel=\"Episode Return\",\n",
    "    filename=\"blackjack_sarsa_learning_curve.pdf\",\n",
    "    xlabel=\"Episode\",\n",
    ")\n",
    "\n",
    "plot_mean_iqr(\n",
    "    returns_mat_ql,\n",
    "    title=\"Blackjack Q-Learning: Return vs Episodes\",\n",
    "    ylabel=\"Episode Return\",\n",
    "    filename=\"blackjack_qlearning_learning_curve.pdf\",\n",
    "    xlabel=\"Episode\",\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ΔQ vs episodes\n",
    "# ---------------------------------------------------------\n",
    "plot_mean_iqr(\n",
    "    deltaQ_mat_sarsa,\n",
    "    title=\"Blackjack SARSA: ΔQ vs Episodes\",\n",
    "    ylabel=\"Max |ΔQ| per Episode\",\n",
    "    filename=\"blackjack_sarsa_deltaQ.pdf\",\n",
    "    xlabel=\"Episode\",\n",
    ")\n",
    "\n",
    "plot_mean_iqr(\n",
    "    deltaQ_mat_ql,\n",
    "    title=\"Blackjack Q-Learning: ΔQ vs Episodes\",\n",
    "    ylabel=\"Max |ΔQ| per Episode\",\n",
    "    filename=\"blackjack_qlearning_deltaQ.pdf\",\n",
    "    xlabel=\"Episode\",\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SARSA vs Q-Learning comparison curve (return vs episodes)\n",
    "# ---------------------------------------------------------\n",
    "plot_comparison_mean(\n",
    "    returns_mat_sarsa,\n",
    "    returns_mat_ql,\n",
    "    title=\"Blackjack: SARSA vs Q-Learning (Return vs Episodes)\",\n",
    "    ylabel=\"Episode Return\",\n",
    "    filename=\"blackjack_sarsa_vs_qlearning_learning_curve.pdf\",\n",
    "    xlabel=\"Episode\",\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Final policy & value maps (from Q-Learning Q-table)\n",
    "# ---------------------------------------------------------\n",
    "# Use first seed's Q-table for Q-Learning\n",
    "Q_ql_example = qlearning_results[0][\"Q\"]\n",
    "V_ql_example, pi_ql_example = q_to_v_pi_from_dict(Q_ql_example, states, actions)\n",
    "\n",
    "plot_value_heatmap(\n",
    "    V_ql_example,\n",
    "    filename=\"blackjack_qlearning_value_heatmap.pdf\",\n",
    "    title=\"Blackjack Q-Learning: Final Value Map (max_a Q)\",\n",
    ")\n",
    "\n",
    "plot_policy_heatmap(\n",
    "    pi_ql_example,\n",
    "    filename=\"blackjack_qlearning_policy_heatmap.pdf\",\n",
    "    title=\"Blackjack Q-Learning: Final Policy Map (argmax_a Q)\",\n",
    ")\n",
    "\n",
    "# (If you also want SARSA maps, you can still keep your previous SARSA plots)\n",
    "Q_sarsa_example = sarsa_results[0][\"Q\"]\n",
    "V_sarsa_example, pi_sarsa_example = q_to_v_pi_from_dict(Q_sarsa_example, states, actions)\n",
    "\n",
    "plot_value_heatmap(\n",
    "    V_sarsa_example,\n",
    "    filename=\"blackjack_sarsa_value_heatmap.pdf\",\n",
    "    title=\"Blackjack SARSA: Final Value Map (max_a Q)\",\n",
    ")\n",
    "\n",
    "plot_policy_heatmap(\n",
    "    pi_sarsa_example,\n",
    "    filename=\"blackjack_sarsa_policy_heatmap.pdf\",\n",
    "    title=\"Blackjack SARSA: Final Policy Map (argmax_a Q)\",\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Wall-Clock Time per Seed (SARSA and Q-Learning)\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(seed_list, wall_clocks_sarsa, marker='o', label=\"SARSA\")\n",
    "plt.plot(seed_list, wall_clocks_ql, marker='s', label=\"Q-Learning\")\n",
    "plt.xlabel(\"Seed\")\n",
    "plt.ylabel(\"Wall-Clock Time (s)\")\n",
    "plt.title(\"Blackjack: Wall-Clock Time per Seed (SARSA vs Q-Learning)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"blackjack_sarsa_vs_qlearning_wallclock_per_seed.pdf\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"Saved: blackjack_sarsa_vs_qlearning_wallclock_per_seed.pdf\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Text summary\n",
    "# ---------------------------------------------------------\n",
    "mean_return_last_100_sarsa = np.nanmean(returns_mat_sarsa[:, -100:], axis=1)\n",
    "mean_return_last_100_ql = np.nanmean(returns_mat_ql[:, -100:], axis=1)\n",
    "\n",
    "print(\"\\n=== Per-seed mean return over last 100 episodes (SARSA) ===\")\n",
    "for seed, mr in zip(seed_list, mean_return_last_100_sarsa):\n",
    "    print(f\"SARSA Seed {seed}: {mr:.4f}\")\n",
    "\n",
    "print(\"\\n=== Per-seed mean return over last 100 episodes (Q-Learning) ===\")\n",
    "for seed, mr in zip(seed_list, mean_return_last_100_ql):\n",
    "    print(f\"Q-Learning Seed {seed}: {mr:.4f}\")\n",
    "\n",
    "print(\"\\n=== Wall-clock summary (SARSA) ===\")\n",
    "print(f\"Mean wall-clock per seed: {wall_clocks_sarsa.mean():.4f}s ± {wall_clocks_sarsa.std():.4f}s\")\n",
    "print(f\"Total wall-clock over all seeds: {wall_clocks_sarsa.sum():.4f}s\")\n",
    "\n",
    "print(\"\\n=== Wall-clock summary (Q-Learning) ===\")\n",
    "print(f\"Mean wall-clock per seed: {wall_clocks_ql.mean():.4f}s ± {wall_clocks_ql.std():.4f}s\")\n",
    "print(f\"Total wall-clock over all seeds: {wall_clocks_ql.sum():.4f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl_report_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
